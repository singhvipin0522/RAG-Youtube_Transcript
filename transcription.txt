Hi, so today I want to show you one of the hottest skills in 2024, which is how to build retrieval of mental generation systems using Python, using the OpenAI API, specifically we're going to be using the GPT 3.5 Turbo model and how to use LAN chain. And the reason I picked LAN chain is because it's by far the most popular framework to build LLM applications. Now by the end of this video, I not only want you to understand how to put together the code, but I want to explain, I want you to leave understanding the reasoning behind every single component that we use to build this solution. So let me go here, let me show you really quick what you are going to find. I'm going to be using Visual Studio Code throughout this video to run the code. The code is in a Jupyter notebook. Now I'm using the extension, the Jupyter extension within Visual Studio Code to run my code within Visual Studio. But if you want to run it outside, you can just install Jupyter and do it outside. You will find a link in the description of this video for the repo containing all of the code. This is the repository here. There is a quick setup instruction. It's very simple to follow. So basically the first step will be installing a Python environment, a Visual environment, and then running the requirements. There are a bunch of libraries that we're going to need to run. So all of them are listed within the requirements, the XT file. The second step is just to create a Pinecon account. I'm going to explain what Pinecon is when we get to that point and then copy the API key. And finally is to create a .m file. So it's an environment file where you are going to host or save the open API API key, the Pinecon API key, and the Pinecon API environment, you are going to get those values from within Pinecon. Now this obviously we're going to be using the GPT 3.5 Turbo, which is very cheap, but you're going to need an API account with OpenAI, and you're going to need access to GPT in order to do this. You probably are going to have to have a credit card there to do this. I promised just running this code, it's just going to cost you sense if anything. It's just very light, but just be aware of that. So after you do the installation, you can open this rack file, and this is sort of like the notebook that is going to show you step by step where we are going to be building here. So here, this diagram here represents the application that I want to solve together with you. It's a very simple application. We're going to start with a YouTube video, any YouTube video, and what I want to do is be able to use a model, an LLM model, or the GPT 3.5 model. I want to be able to ask questions about that video. Now you can imagine a student that has the videos of a class that's going through, and the student wants to just ask questions that are going to be answered from the content of the video. That's the application that we're going to build together here. So at the end of today, you're going to be able to use your own video, whatever video you want. That's what you're going to be able to use. At 40's example, I'm using this particular interview with Andrew Karpati, Alex Freeman. It's a three hours and a half interview. So it's a long, long video. And what we're going to be doing is asking questions from this interview. So that's sort of like the process. So going back to this diagram, you'll see the idea of how to do this. It's kind of simple. We're going to grab the YouTube video. We're going to generate a transcription out of that YouTube video. We're going to get all of the text, the entire transcription of the video. And together with a specific question, we can give those two pieces of information to a model in order to get an answer. So the idea would be something like, hey, what is the, I don't know, what is artificial intelligence that will be the question. And then answer the question from this transcript. So we're going to give the entire context of the video or content of the video to the model. And the model will look up what artificial intelligence within that transcript and will give us an answer. So very, very simple. So how do we accomplish this? How do we do this? Well, I'm going to start this notebook here by just loading using the .im library here. I'm going to be loading the environment variables in memory. Remember, we added the open AI API key to those variables. So I need to load them. That's what I'm loading here because I'm going to be setting up a model. And I'm going to need that API key. And this is the URL of the video. Again, this is the interview between Lex Freeman and Andrew Karpati. That's the video that I have here. You can use your own video. Okay, so I'm going to load this. All good. And now I'm going to set up the model. Okay, so this is the main component that we're going to be using. If I go back to the diagram that I have here, that's this blue square here. That's going to be our model. Like I said, before, we're going to be using the GPT 3.5 turbo model. You can use GPT 4 if you wanted to. GPT 3.5 is good enough here. Notice I'm using Langchain here for the first time. Langchain supports multiple models. Like you can use Langchain with any sort of model. You can use it with Lama, OpenSource models. You can use it with Extral. You can use it with Gemini. You can use it with any sort of model. Specifically here, I'm using the chat OpenAI class. It's a pretty fine. It's an out-of-the-box class that's going to give me access to OpenAI's models. And I'm initializing that class with the key and with the name of the model. Okay, so after running this, now we have a variable here, a variable model, that we give us access direct access to that model. So let's try it out. Let's test it. So I can just invoke that model with just a question. And this is just to see whether the model is actually working or not. You can put here any question. I specifically run it using a question what MLB team won the World Series during the COVID-19 pandemic. And you can see here the answer coming back from the model is an AI message with a content inside that says the Los Angeles Dodgers won the World Series. Blah blah blah blah blah. Okay, that answer is correct. So this model is clearly working. You're going to notice something here that's going to be the same for many different components from LineChain. And it's that invoke function. So all of these components we can call them using an invoke function. And we're going to pass a query inside. And those components are going to do what they're supposed to do. All right, so so far we have a model and we can ask anything we want to that model. Just let me add just a new line here. I'm going to add something in Vogue. How much is 2 plus 2? Just just to make sure this is actually working and you get here back. Okay, so the content is 2 plus 2 equals 4. Okay, so this model is clearly working. That's fine. The only thing that I don't like at this point is that as you can see the the output coming from the model is an AI message. So it's it's it's an object. What's coming back? I would like just to get a string back. Unfortunately, LineChain supports the concept of a parser. And this is the way it works. It's just very, very simple. We have our query, which is how much is 2 plus 2. We send that to the model. The model is going to give us back a response and we can take that response through a parser, which is going to be a class that's going to decide how to format how to output that result. Okay, in my case here for for what we care about, I'm going to be using a string output parser, which is the name suggests is very, very simple. It's a very simple class that is going to take that AI message instance and turn it into just a string. Okay, and here for the first time we start seeing the main idea of LineChain, which is precisely the chaining portion of their name. So we can start chaining components together in order to accomplish something or a task that's a little bit more complex than what a single component can do by itself. So notice here, I'm creating the parser and it's just a string output parser. Again, this is just going to take whatever the input is and turn it into a flat string. And then I'm going to create a chain. And a chain is putting together the model connected to the parser. And this is the symbol in Python, the pipe symbol is how we're going to make that connection. So I'm saying, okay, just call the model and take the output of that model. And that's going to be the input of the next component. In this case, it's going to be the parser. And let's ask the question again. So now we're going to invoke it with the same question we asked before. What I'm going to team one the world series. And when we run this, now we are going to get the same answer back. But now notice is just a string. It's not an AI message anymore. It's just a string. And this is going to make things a little bit clearer. Okay. All right. So let's take this to another level. If we go back up and we see the diagram, what we want to send to the model are two pieces of information. We want to send a question, but we also want to send a transcript. So we want to ask the model and show the question from this transcript or this context, right? So we need to combine those two pieces of information in the prompt that we're going to use with the model. Okay. So in order to do that is I keep write that just in a string, but we're going to introduce a new component from Lanchane in order to set up a prompt. It's going to be a little bit more complex than just a string. So here is our prompt. And because the GPT 3.5 turbo model is a chat model, it's not a completion model, but a chat model. I'm going to be using the chat prompt template class from Lanchane. Again, Lanchane supports a bunch of prompts. By the way, here I have the Lanchane documentation with all of the models here, the modules here. And you can go through the modules and you can find everything that the day support just going to focus on the ones that I'm using here. So the chat prompt template here is the string that I'm going to be using with the model. It's very simple. It says, answer the question based on the context below. If you cannot answer the question, reply, I don't know. So that is, is that simple. And then I give the model the context. We're going to be using this context to inject the transcript from the video. And I'm going to be giving the model a question. Okay. So very, very simple. This is my template string. And now I can create my prompt object, which is a chat prompt template from this string here. Notice the syntax that I'm using to specify context and question. That syntax is telling Lanchane to create two input variables. And you're going to see how we pass those right here. Notice that now I can say prompt, just the variable that we just created format. And then I pass the context. And then I pass a question. Okay. So these two correspond to these two variables that now I can pass it, just two arguments or parameters that now I can use together with the format function in order to generate the full template. Okay. Now I'm using here sample context. I'm saying Mary's sister is Susana. And the question would be who is Mary's sister. And again, this is just sample. And when we execute this, let me do it out again, you're going to see the output from that prompt or not the output. Remember here we're not talking to the model. We're just generating format in that prompt. It says, well, the human is saying answer the question based on the context below blah, blah, blah. And then over here, you're going to see the context says Mary's sister is Susana. And then the question is going to be who is Mary's sister. So it's sort of like putting together or replacing those variables context and question by the values that we are passing here. So very straightforward. This is good. So if we shame now together, if we take that prompt and shame it together with the model and with the parser, we're going to get something that looks like this. We're going to get something that looks like this. I apologize about that. I hit the microphone for some reason. So we're going to get the prompt, right, expecting two parameters, the question and the context, that prompt, the output of that prompt, which is basically formatting the question, the context with the with the overall template. That output is going to go to the model that's going to be the query that we're going to send the model. The model is going to process that query. It's going to take the response, take it to a parser. And finally, that parser is going to parse out that answer that what we want. So this is sort of like the full chain so far. So let's see that chain in action. You're going to see that I'm going to be using the Pyke symbol, like with it before. I'm going to be chaining the prompt with the model and then with the parser. And again, the output of the prompt becomes the input of the model, the output of the model becomes the input of the parser. And when I invoke the chain now, we need to pass two parameters because the prompt is expecting those two parameters. So I need to supply those two parameters to the prompt. So I'm going to invoke the chain with the parameter context. When I say, Hey, Mary's sister is Susana. That's the example context that I'm showing the model. And a question, and that question is who is Mary's sister? And now when we run this, you'll see that the model is returning the right answer. So the model is basically what we sent to the model is, Hey, answer this question from the from the context below. And the context is Mary's sister is Susana. And the question was who is Mary's sister? The model is just going to process that and it's going to say Susana. Pretty awesome. This is how little it takes, right? So obviously, after defining the prompt, the model and the parser, my shame is just I put in together every single component. Okay, so this is good. Before we get into the transcript of the video, let me show you how powerful chains can get because you can now combine different chains together. You can create very complex systems by combining different chains together. So I have this section here. Again, this is like a like a small rabbit hole. It doesn't necessarily is going to help us solve the YouTube problem. But I wanted to show the power of the chains here. I'm creating here a new prompt template that I'm calling translation prompt. We did, we already have a prompt, which is the prompt of answer from the context below. This is a new from a template that says translate one parameter answer to another parameter, which is language. So this is a new prompt that I just created here. Okay, and now I'm going to combine this prompt with a model to perform what the prompt is doing with the previous chain. Okay, so this is what that is going to look like. Looks like a little bit complex, but I promise is very simple. So we have our original chain at the top. Question context goes into a prompt, the prompt goes into a model, the model goes into a parser. But now I'm going to use the output from the parser, just that answer that's coming from the parser. And that is going to be one of the inputs of my second chain that I'm going to create now. So you will see the answer going into the translation prompt, just the one that I just show you. And I'm going to need to specify a language as well. So I specify a language. I'm going to use the answer coming from the previous chain. And then I'm going to take that through a model. I'm going to get the answer from the model. And I'm going to parse that answer out to get my final response. So if you have a notice, already, what I'm doing here is I'm creating a translation chain or I'm going to be creating a translation chain that is going to translate the answer from the first chain into any language I want. Okay, so this is what that translation chain is going to look like. I have an answer and I have a language. These are the parameters that I'm going to be expecting from outside. Okay, and this is sort of like a little bit of syntax sugar to specify parameters here. A part of this chain, notice how I'm using like the curly braces. The answer parameter is coming from chain and chain is the previous chain that we defined before. Okay, that is where the answer is coming from. And then I have language, which is coming from and I'm just using the item getter function to grab the language from the invocation of the chain. So whenever you invoke a chain and you pass parameters, if I use item getter here, I can just grab that that parameter from the invocation of the chain. So these are my two parameters and then I'm going to get all of these two things. I'm going to get them into the translation prompt. Okay, which is the prompt that says translate this whatever we get here into this language here. Okay, that's my translation prompt with a finite before. And then I'm going to take that into the model, which is the GPT 3.5. And then I'm going to take that through the string output parser. Okay, and now I'm going to invoke the chain. So the context, why do I need a context here? Well, remember we are we are changing a chain. So we still need to pass the context here. So that context gets used by the original chain. The context says Mary sister is Usana. She doesn't have any more siblings. Okay, the question is how many sisters does Mary have? And the language is Spanish. Okay, notice I have three inputs. Let's check the diagram here really quick. You're going to see the question, the context and the language. Those are the three inputs that my overall chain is expecting like the big one here. Two of them will go to the previous chain. The third one will go to the translation chain. Why I don't need an input for answer because answer is coming from the output of the first chain. All right, so let me run this. Just make sure it still works. It doesn't. It says that translation prompt is not defined. That is because I forgot to run this line here. Do that now. We run this. And here you go. Maria tiene una hermana. Susana. Very nice way of answering this question, but it's correct. So just here, hopefully this makes clear how you can have separate chains doing different things and you can connect them together in order to accomplish something that is much more complex, which is I think is really cool. All right, let's get back to the YouTube video problem that we have. And so far what we have right now is we have a template that expects two variables, the question and the context. We have a model. We have that template connected to a model, and that model connected to a parser, to an output parser. So I think those are the big components that we need. What we need to do right now is just grab the YouTube video, transcribe that YouTube video, get the text and use the text as the context. And that's it. That's pretty much it, right? Well, not really, but we're going to get there. So to transcribe the video, I'm not going to spend too much time on how to transcribe the video. I'm going to explain this really quick, but basically I'm using the Whisper library. Whisper is, you know, you can get a link here. So let me just open this link. I don't even know how to open a link here. Yeah, let me grab this link. I don't want to click on the link because it's going to open Chrome. And I'm going to show you. So here is the Whisper library. Again, Whisper is open source. You can breathe here. The whole explanation of how it works. It just works great. In my experiences, it's very, very good. I want it does. So I really don't have any to use anything else. So the way the code is working is I'm first checking whether transcription.txt exists. Again, this is a three and a half hour video. So I don't want to transcribe the video over and over again. So I'm going to check if the transcription exists. If it doesn't, then I'm going to download the video and transcribe the video. If it does already, if it exists already, I'm just going to skip all of this. That's what this line is doing. I'm using the YouTube class from Pi YouTube. It's just a library that you can install. We'll give you access to YouTube videos. So I'm just initializing the YouTube class here or the YouTube instance here. And then I'm going to grab the audio from the YouTube. That's the only thing I care. I don't care about the video, only the audio. So I'm going to grab the audio. I'm going to initialize the whisper model. I'm using the base model. This is not the most accurate model that whisper has to offer, but it's very, very good at a very decent speed. If I were to use an even more accurate model, it's going to take longer to transcribe the video. So again, depending on what you're doing, just consider that there are models that are going to be even more accurate than the one that I'm using here. So I'm initializing or creating the whisper model here. And then I'm going to just download the audio to a file. And I'm going to transcribe the whisper function called transcribe. Whisper has to support that function. I'm going to transcribe that file. And I'm just going to grab the text and save it to a file. So if I open the file, you'll see it here. I have it because I did it before. You will see this is just is obviously three and a half hour conversation. So it's a block of text with all the entire conversation between corpati and Alex. All right. So that's awesome. I have the transcription. So here I'm showing you. So I'm going to run this. Not going to do anything because the file is in there. This cell here opens the file and just displays the first 100 characters just to make sure that everything is in there. Good. And now I'm going to be invoking my chain. So very simple. I'm going to grab my chain. We'll let it define that before. And I'm going to invoke the chain passing my context, which is going to be the whole transcription. And passing the question and the question is, is reading papers a good idea? They talked about that. So I want to know what the opinion is about that. And notice that I added a try except exception here for a good reason. When I run this, what is going to happen is that we are going to get a 400 error code back. So it's not going to work. Now the message is where you're going to find the clue of what's happening here. It says this model's maximum context length is 16,000 tokens, 16,000 385 tokens. However, your message resulted in 47,000 tokens. Like, I don't know, like three times more tokens than what the model supports. Please reduce the length of the message. In English, what this is telling us is the transcription is too long. And the model does not support such a long transcription. Now the error talks about tokens. Let me explain really, really quick without this being a video about tokenization. Let me explain what this is. Remember, these models that are processing text are using behind the scenes. It's just a big neural network. And we have to find a way to vectorize the text, turn text into numbers for the neural network to work. Like the neural network is not going to work directly with letters, with characters, right? It's going to work with numbers. So we have to find that translation between the text and the numbers. And that it's using a process or we use a process to do the translation or that conversion that's called tokenization, where we tokenize text. Now here's a link, and I added that link here in the in the notebook. I'm going to show you here. This is tick tokenizer. It's just a website that's published online that you can use. And specifically, I'm going to be using the CL100K-based tokenizer, which is the one that's using OpenAI behind the scenes. And notice here that hello, this is the year 2024. I'm just typing a sentence. And over here on your right side, you're going to see how many tokens come out of this sentence. There's this hello, this is the year 2024. Notice that with colors, you get what the tokens are, right? So 9906 is going to be the word hello. 420 is going to be space this, right? 374 is going to be space is, right? Now notice, this is funny. The year 2024 is not going to be tokenized as a single value. Instead, there are two tokens for the year 2024. You add 202, okay? Which is 2366. And you get 4, which is 19, okay? And there are a bunch of quirky situation. Is this correct? Is this and this is correct? All right. So a bunch of ways to represent things here. Notice this. So is lowercase or capital, capitalize is, is going to be 3957. All capital is is a different token in 1669. Space is is 374. So notice these three is concepts are represented with different tokens here. All right. Now there are multiple reasons for that to happen. You can play with this tokenizer here to see how it works or why not. This is what's important just to remember. As a general rule, think of tokens at 75% of what the number of words that you have. So for a thousand words, you can count approximately, they're going to be like 750 tokens approximately. So that's like roughly what it comes out to after you tokenize a text. But when the model, when we get an error saying that the model has a maximum context length of 16,000 tokens, you can do the math there and determine how many words you can actually fit. Remember, that's how big that prompt is going to be when we send it to the model. Obviously, in a three and a half hour video, there is just too much context. And you can go beyond YouTube videos. You can think about a company, which is a very, very common request right now. They want to process their knowledge base with a large language model. But they have a gigabyte of data. Not only three and a half hour transcript. They have gigabytes of data that they want to process with a large language model. Let's say they're creating a chatbot for their customers to ask questions about the company, about the products of the company. The company cannot fit their entire knowledge base in a single prompt. So they need to find ways to split, to chunk their entire knowledge base, split that into most relevant context. Like if you were doing this information, or if you were doing this problem, sorry, and you wanted to solve it manually, what would be the first idea that comes to mind? In my case, well, obviously, we cannot send the whole transcript to the model. We need to somehow send only a portion of the transcript that makes sense for the model to use to answer that question. Okay. So let's say we have two classes. One class is a math class. And another class is a history class. And both of those classes are within our knowledge base. If the user asks a question about math, we don't need to send the entire class of the entire history class to the model. We will only send the math class. And if the user asks anything that's related to history, then we will only send the history class. So we need to find a way within this transcription to somehow select, find, and select the portion of the transcription that could potentially be helpful for the model to answer the question. And that is precisely what makes Rack applications a little bit more complex to build. That selection process, that chunking and selection processes, what makes these applications a little bit more complex. So this is sort of like this idea represented here in a diagram, right? We want to send the question to our model. But somehow we want to take that transcript, split it into documents, and only send the document to the model that is relevant for that question. That is our challenge right now. How do we do that? Okay. That is what we want to do. All right. So in order to get there, the first thing that we need to do, we obviously need to split our transcript. We cannot work with the whole thing. So let's find a way to just chunk it out, just split it into separate portions. So to get there, I'm going to be using a text loader. So we're working with a text file. So I'm just going to use a simple text loader. This is another component from Lanchane. And as you can see, the text loader is just simply loading that file in memory. It's going to be easier for me to work like connecting this text loader later to a splitter. So we can actually split the text. So I'm going to load this text document or this file into a variable called text documents. I'm printing out here text documents. Let me run this. And you're going to see it says, well, there is a document here. And the content is going to be the entire transcript. Okay. You know, I can keep sort of like scrolling and I see the entire transcript. That's what happens when I load this text document in memory. Now we need to find a way to split it. And there are many, many different mechanisms to splitter to split a text. I have a link here. Let me open that link. Really quick. So if I open this link, I'm going to open it up here. All right. You're going to find the types of text splitters that you can use that are supported by Lanchane. And there is recursive. There is HTML if you're using HTML documents and you want to split HTML documents while the HTML text splitter knows about characters knows about HTML characters. So it's a good idea to use it. You get marked down. If you're processing code files, you get a code splitter supports Python and JavaScript, right? You get a token, a character, and you get a semantic chunker. This is a very interesting one. Okay. So this just splits the document in a smart way. Again, it's experimental. But anyway, here you can find the entire list of splitters. And we're going to be using a very simple one. We are going to be using, let me just execute this. We're going to be using a recursive character text splitter. So this is the way it works. Okay. We're going to take our entire transcript. And we're going to specify a predefined length, like how many characters we want on every chunk or every document. And then we're going to specify a little bit of overlap. So we're going to get, let's say, from the characters 0 to character 100. And that's going to be the first document. And then we're going to go back 20 characters that's going to be our overlap and go from character 80 to character 180. And then go back 20 characters and do the same thing. So we're going to have that overlap between documents. Here is the example. Let me run this. I'm going to use the recursive character text splitter. And I'm specifying a chunk size of 100. And 100 is just too small, but just to show you here how it works. I'm going to probably set it to a thousand later. But you're going to see chunk size 100 and the overlap of 20. And I'm going to split the document that we loaded here, which is just a big document at this point. And then I'm just displaying the last five or the first five documents here that we created. So notice this. It says, I think it's possible that physics has exploits. That's how this document starts. Notice that the document ends saying to find them arranging some and the document ends there. And if we go to the second document, it says arranging some kind of a crazy quantum. See how there is an overlap of 20 characters there. Let's see how this document ends. The second document ends with you buffer overflow. And then the third document says buffer overflow somehow gives you a rounding. So that is the way I'm splitting this document. Okay, so I have a huge transcript that I'm going to be going and chunking out into documents of specifying length. Now a hundred here is just for me to illustrate what this looks like. But a hundred is probably not enough. So I'm just doing a thousand here. Just execute the same thing. But now doing it a thousand. You can try or this is going to be a hyperparameter of your system that you're going to have to try an experiment with. If you're going to be using recursive character text later, you might want to try with 2000, maybe 3000, change the overlap depending on your document. This is something that you're going to have to try it out. All right, so having done that, now what we have is what we want it. So we have a bunch of smaller documents and each one of those documents will fit the context, which is good, right? We don't need to send the transcript anymore. The problem, however, is that we need to understand, we shift those documents to send to the model. Like we get a question about artificial intelligence. How do we know which of those smaller 1000 word documents are the ones that we need to combine and send to the model? That is the question that we need to answer next. And this is one of the most fascinating topics of working with large language models and working on these type of systems that you are going to find. And it's the idea of embeddings. So let me show you first what this looks like here. And you're going to have to trust me for one second. And then I'm going to make this a little bit more clear. So this is my proposal. So if we start with the transcript and we split it into documents, we already did that. And we have all of those smaller documents. Imagine that there is a magic formula for us to compare a question with all of those documents. And that magic formula that I'm calling computing similarity here, that magic formula will tell us which of those smaller documents are the most similar to the question that the user asked. And using that similarity, we could return the documents that are the most similar. So we could select the documents that we want to send us context to the model. So the process of doing that, I'm saying this magic formula, this actually exists, we can generate embeddings for every one of those documents and generating embeddings for the question. Now what an embedding is, is just a vector in space, you can think of coordinates in a multi-dimensional space of where a specific idea is located. Okay, so here is the way it works. Imagine that I talk about books, I grab a book, this is the tip learning with Python book from François Shoulet. And I'm talking about books. And I generate the location in this multi-dimensional space of where you would put this book. And that book will live here in this location here, I don't know if you can see it in the camera, probably can. But imagine, actually, let me just, let me just put this here, let's say I locate this book that corner over there. Okay, that is the coordinates where this book is going to be located as a concept in my world. And now I grab a can, okay, so I grab a can and this is a very different concept. So it's not going to go close to the book, it's probably going to go to a different corner of my room. So the coordinates representing this can are going to go somewhere else. And now I'm going to and not a book, this is a human in the loop book. This is a very similar concept to this book. So if I were to locate this, if I were to generate coordinates for this book, those coordinates will probably place these two books very close to each other over that corner. I'm very far apart from the can that's located on that. So hopefully these are like three DIDI make sense. But the embedding is basically a function, for now we can call it a magic function, that given a concept, given an idea, a text, a word, an image, it located, it's all like a generates a coordinate in multidimensional space of where that concept idea object is should appear. And related concepts are going to be located very close to each other. While separate concept concepts that are not similar should be located for apart. So let me give you one specific example. And I'm going to be using for this example, I'm going to be using the cohere playground. Cohere is an amazing company. They have a very good large language model. They have a playground, which is what I'm using here. And that playground, I'm using their embedding section of the playground to generate the location of several ideas that I added here. So you're going to notice I have seven different sentences. Mary's sister is Susana, John and Tommy are brothers. Patricia likes white cars. Pedro's mother is a teacher. Lucía drives an Ari. Mary has two siblings and Mercedes are amazing automobiles. Okay. So seven different sentences. And I generated embeddings for all of these sentences. What cohere is going to do is going to generate those locations, those vectors. But what's really cool about this playground is that those vectors, by the way, they have multiple dimensions because there are those are coordinates in multi dimension space. But cohere is sort of like displaying a compressed version of those vectors in two dimensions. So we can visualize and see what happens with those vectors. So this is the output after we did that. So the first thing that you are going to notice is that there are four sentences toward the left and three sentences toward the right. So let's explore which sentences cohere decided to group together. So sentence number three, Patricia likes white cars is close to Mercedes or amazing automobiles and it's close to Lucía drives an Ari. So the three sentences that somehow talk about cars are close to each other. Now this is what's really cool about this. Okay. Notice that the sentence says Lucía drives an Ari. It doesn't say they were car. It doesn't say they were automobiles. It says Ari. The third sentence uses the words white cars. And somehow cohere's model knows that white cars is a similar concept than Audis or Audis in this case. Same thing with automobiles and Mercedes. See how these three concepts are together. Now let's see this here. You get John and Tommy are brothers and that's close to Mary's sister Isusana. That's close to Mary has to siblings. Notice how these three sentences are close together. We're talking about sisters, brothers, siblings. And then you get Pedro Smothery's a teacher which is a little bit further apart from the civil in idea but still close by. Embeddings are an amazing idea that makes everything that you see here possible. Okay. So if we go back to our diagram, if we generate embeddings and this is the section that says embed here, if we generate, if we can generate embeddings for each one of the documents that we generate it, right? Each one of the chunks we generate embeddings for them. And we also generate the embedding for the question. We can then compute the similarity, how similar, how close are those embeddings? And basically grab the most similar chunks or the most similar documents and use those most similar documents as the context for our model. So if our content was about the question that the user is asking is about cars and we generate the embeddings of all of these questions, then we could potentially send these three chunks, these three sentences, three, five, and seven, to answer a question about cars. If the question was about family relationships, then we would send the closest of these four here or maybe all four if they fit, right? That is the idea behind using embeddings to solve the problem with the transcription. So how do we do that? Well, you'll see. Fortunately, OpenAI or not OpenAI, but Langcheng thought about this and they offer the ability to use embeddings from OpenAI. So they have a class, it's called the OpenAI embeddings that we can use to automatically generate, not automatically, but generate the embeddings for anything, right? So here's one example. I'm creating this instance called embeddings and I'm embedding a query who is married sister, okay? So I'm going to generate this embedding here by calling embedding query and then I'm printing out the result and you'll see this is the embedding and it's just a long, long vector. Remember, this is just a location in a multi-dimensional space of where that query is going to live. Just wanted to show you what is going to generate the length or the number of dimensions is 1536, so 1536, dimensions here. That's how many little numbers here or small numbers here are going to be within that vector. Now that that is interesting, we're going to use that later, but for now, we have a way using OpenAI or using Langcheng to generate the embeddings. Let's see a little bit more. Let's see how these embeddings work now. Just to make sure we have all of the ideas that we need to solve this problem. I'm going to generate now two more embeddings. In this case, embeddings for two separate sentences, okay? So the query was who is married sister. The sentences are married sister is Susana and Pedro's mother is teacher, okay? So obviously I generated three embeddings. What I want to show you is how we can compute how close these embeddings are to each other and if everything is working as it should, then this, the embedding for who is married sister, should be closer to married sister is Susana. Because that is the answer that direct answer to the question that I'm asking. How do I do that? Well, I'm going to be using cosine similarity. There are many different ways to compute, to take two vectors or two embeddings and compute how close they are to each other. The most popular way by far is cosine similarity. So I'm using cycle learn. Cycle learn provides cosine similarity functions, so I don't have to implement it. It's not hard to implement, but anyway, I don't have to. And then I'm going to be computing the cosine similarity between the query, the embedded query and sentence one and the embedded query and sentence two, okay? So these are the two similarities and when I print that out, let me execute this line by the way, when I print that out, you're going to see that the similarity between the query with married sister, with the first sentence, the first sentence was married sister is Susana, the similarity was 0.91. Remember, similarity of one will be perfect, zero will be very, very dissimilar and the similarity of the second sentence is 0.76. So clearly, the embeddings are working. I'm getting that married sister is Susana is the correct answer or it's the closest, not the correct answer, I'm sorry, is the closest of the two sentences to the query that I asked. Okay, awesome. So how do we use the concept of embeddings, right? How do we use we already know that we can use embeddings? We already have a chain, but we need to find a way to take that transcript and put it all together with our chain so the process works without any interruptions. So there is one more problem that we need to solve and is that if you think about it, if you get a three and a half hour transcript, that's a huge amount of content and we split that, we have many, many different documents, potentially thousands of documents that we're going to have to go through and compute similarity when every time we answer a question, we're going to have to compute the embedding of the question, compute the embeddings of every single document and then find which of those documents are the most similar. There is a lot of process in there. Fortunately, we don't have to do any of that by hand because there is a new idea, it's called a vector store, a new idea here for us in the video, obviously, it's a vector store which is a database for vectors, a database for embeddings. That's very simple. And the whole idea of a database for embeddings is that we can use that database number one to store all of our content, all of the documents from that transcription number two to automatically generate the embeddings for all of those documents and store those embeddings so we don't have to regenerate them with every question. And number three and it's the most important one, vector stores are optimized to do similarity search really, really quick. So that means that we can give a vector store an embedding and say, find me these many documents or the top three documents that are the closest to this embedding and the vector store will do that really, really quick, which is precisely what we need for this, so let's set up a really quick vector store. This is how the process is going to look, right? We're going to get our transcription splitted into documents, we already did that. We're going to get all of those documents and we're going to generate the embeddings for all of those documents. That's going to happen behind the scenes. All of those embeddings are going to get stored in a vector store and then that vector store is the one that we're going to use to get the question and produce the most similar chunks or the most similar documents from that list, okay? So for starters and just for this example here, we're going to take it to Pineco in a second, I'm going to be using a Docker rate in memory search vector store. This is just a vector store that is kind of working memory, okay, in the memory of your computer. So I will have anything else to set up and just for the example, I'm going to use the same seven sentences that I showed you in the coherent playground. So these are the seven sentences that I'm loading into my vector memory vector store. So not this here, I'm going to run this, how I'm saying, hey, just generate this memory search from these texts. So I'm just loading that into memory and I'm using this is important. I'm telling this vector store which class it should use to generate the embeddings. And because we're using the OpenAI embeddings here, we're using the OpenAI and BI, we need to use the embedding model from OpenAI. If you were using a Lama model, you will need to use a different way to generate embeddings. If you were using the cohere model, you will need to generate embeddings with the cohere embedding model, okay? In this case, I'm using the embedding models from the OpenAI API. Cool. So after executing this, I have vector store one, that's the name of the variable, which is the vector store. And now, notice this, I can just call similarity search with score, just a function. And I can pass a query who is Mary's sister. And I can specify how many chunks I want back. By the fall, it's going to return four. I'm just specifying three here just for the example, but just so you know, by the fall, you're going to get four, the top four most similar documents back. So I'm saying who is Mary's sister? And I'm getting in order, Mary's sister is Susana, Mary has two siblings, John and Tommy are brothers. Those are the three most similar documents from this vector store to this question here. Awesome. All right. So how do we connect this vector store to the previous chain? So we're going to need a new concept that's called a retriever, okay? Very simple. You'll see how it works right now. So this is, remember that our previous chain required two parameters, required a context, and it required a question. And remember, our previous chain started with the prompt and we passed that context and we passed that question. Now it's just going to look a little bit different. So now the way it's going to look is, well, we're going to add a retriever in front of the prompt. That retriever is the class that will take care of connecting to the vector store and retrieving the documents that will become the context, okay? That's why you see here that the context is coming now from the retriever into the prompt, okay? So let me show you the code and how to create, how to set up a retriever really, really quick. From any vector store here, you can just generate a retriever. By the way, there are multiple retrievers are not only connected to vector store. You can have a retriever that's going to run a different algorithm behind the scenes to select different documents. Like for example, like a page rank algorithm to do search like Google does, for example, and select documents using that algorithm. In this case, I'm generating my retriever directly from the vector store. So I'm saying, hey, off of this vector store one, just gave me a retriever. And now if I invoke my retriever with a question, the retriever behind the scenes will take care of everything. The retriever is generating an embedding for that question, is sending that to the vector store, and is retrieving the top four documents and sending them back. That's all of that is done by the retriever. The retriever is sort of like the gateway so I can connect it to a chain, and the retriever is the gateway to that vector store. Not as here, who is Mary's sister? This returning, I'm going to just run it. It's returning Mary's sister is Susana, Mary has two siblings, Jan and Tommy are brothers, etc. Okay, so the retriever is working. Now we need to connect that retriever to the prompt. Okay, the prompt expects two parameters, the context and the question. Okay, so this is how you can connect the retriever to the prompt. Remember that before when we generated the translation prompt, I show you a little bit of syntax sugar to do curly braces to specify the parameters. Here I'm just going to unravel that a bit. We're going to be using a class that's called RunablePadalow, which is a Lanchang class that is going to let us do two things at the same time. Okay, so the two things are very simple in this case. I'm calling this a setup, and the setup is, well, the context is going to come from the retriever one, that's the one that we just created. That's where the context is coming from, okay, from the retriever. And the question is going to be a pass through. So I'm taking the question, I need to specify the question because the retriever, it's using the question, so it needs the question. But I'm also telling this RunablePadalow class to pass that question through the next step in the process because the prompt requires that question as well. Retriever requires the question because it needs to generate the similar documents. But the prompt, the following component on the chain where we connect this retriever to the chain, we need that question. Therefore, I'm going to do a RunablePadalow pass through, okay. And now you can see here how this RunablePadalow pass through is just going to return a map exactly what my prompt is expecting. So I invoke it with what's a great car, tell me what what what's a great car. And you're going to see that the context that it returns is Mercedes are amazing automobiles. Notice how I have to just just just one second appreciate how good embeddings are when I'm asking and using the words great and car and the idea is well Mercedes instead of car and automobiles instead of car and amazing instead of great and still these two concepts are very close to each other. Just just close parentheses there on my yeah I'm used meant. So anyway when I invoke this RunablePadalow class with that question I'm going to get a map back with a context variable containing an array with all the four documents that are the most similar and a question and the reason the question is coming back is because it's a RunablePadalow pass through and because it's a RunablePadalow pass through it's just going to pass it through the output as well and the question is what's a great car. Alright so now I can get this setup and put it together as part of my chain, okay. It's very simple now I can just I have my chain here and I can at the setup connected to the prompt connected to the model connected to the parser and I can ask my question what color is Patricia's car and if I run this it's going to say it's white and it's getting that so it's going to the setup is going to the vector store remember the setup is going to call the retriever it's connects to the vector store find the top documents that are the most important ones to answer this particular question then is taking that connecting all of that into the prompt template. The prompt template going to the model the model outputting into the parser the parser is just giving me that final string. The second question is what's a great car and then the answer is going to be Mercedes so it's getting that answer from the prompt. Alright so all of this is good but we really need to do this with the transcription which is the huge document right now it's just just sample so let's load the transcription into the vector store this is going to be very very simple at this point. I have my vector store in memory class here and there is a from documents and I can pass just the list of chunks that we created remember that we split the whole transcription into multiple documents well I can just pass that here by the way let me just check because I don't know what is the length of document like how many chunks do we have we have 221 chunks and just to make sure that we're working here with what I want I just want to see just documents here I think it's possible that physics has exploits blah blah blah so that's the first document here. Alright so awesome so 221 documents I'm loading into a new vector store that I'm calling vector store 2 okay and remember I need to pass what are the embeddings that I'm going to be using how am I going to be generating those embeddings what is the class and these embeddings is the open AI embeddings that we generate it. Cool awesome so now that I have here this is the syntax the syntax sugar back again instead of using the runable parallel I could use the runable parallel but instead of using that I'm just going to be using this quickly thingy here the squiggly braces here notice that I'm saying okay so I need the context is going to come from the the retriever that I'm generating from vector store 2 and the question is a runable pass through because I need to pass that to the prompt as well then connected to the prompt then connected to the model then connected to the parser okay and by the way this is exactly the same and I'm just going to show you here just so it's clear so this here that we just did is exactly the same as if we do set up two frontable parallel context yes this is cool co-pilot generated all of that code really quick so I'm going to put a set of two. Alright so these two are equivalent I'm just showing you a little bit of syntax sugar on how to do that let me run this here well maybe not because vector store 2 is not doesn't exist because I need to run this first okay now let me run it okay so this is my chain and actually I need to invoke the chain if I want something to happen I'm going to need to invoke the chain what card does you see drive let me ask a question it says I don't know that's that's awesome of course not because this is using the vector store 2 which is the vector stores that's connected to the transcript so let me just do something what is a GI let's see I don't know I don't even know if they answered this question a GI stands for artificial general intelligence okay that's good and let's do the second chain again this is exactly the same thing as I did before but instead of using the runable parallel class directly I'm just using the syntax sugar with the squiggly braces and what is synthetic synthetic intelligence when I run that and then it's going to answer synthetic intelligence is described as the next stage of development in the context blah blah blah this answer is coming from the transcript now we have it in this vector store cool we are almost done just one more step and it's that that vector store is in memory at this point we don't want that vector store to be in memory we want it to be physically store somewhere in an actual vector database there are many many different vector stores that you can use here I'm going to be using pine cone it's very popular so I created a pine cone account before recording this video you can see it here and I have a project that I call YouTube and I'm going to create an index here it's really fast I'm going to create an index and I'm going to call it YouTube index okay so I'm going to call it YouTube index what does it say can only contain lowercase letters deny YouTube okay YouTube index that's going to be the name of my index I'm going to copy that because I'm going to net it and then it says how many dimensions am I going to use remember is 1536 dimensions you can also go here setup by model and you can find this is the embedding model that we're using so it's 1536 dimensions this is the number of dimensions the embedding the embeddings have okay so because we're using the open AI we have 1536 dimensions okay awesome so this is I'm going to click create index this should take just a second to have this index ready there we go not records yet awesome I'm going to go now back to my code and here in my code you're going to find a variable that's called index name and I need to paste here the name of the index that I use which is YouTube index and I'm going to run this cell and what this cell is going to do notice we already did this before for the memory store the memory vector store where we loaded all of the documents that we generated in that vector store in this case I need to specify the documents that I'm going to be loading which is the transcript chunks from the transcript the embedding model that I'm going to be using and then index name so I'm going to do this this is loading this is going to take a few seconds and that's it 3.5 seconds is done let's go back to pine cone and if I refresh this screen we should see hopefully there we go so we should see all of the vectors are ready here in this index so notice that you get the source just a txt file the text this is the entire text of this chunk or the original document this is the content that we give the model later and we also get the vector which I don't see here it might be you know what it might be because it's well maybe not it doesn't show it doesn't show like the actual vector values not that that is important but anyway so I have everything here in pine cone so now the rest of the code is just the same that we saw before right I can show you that this is working by just running a similarity search over pine cone we already did this before with the memory database and do it what is Hollywood going to start doing that is just a question I'm basically saying return documents that are similar to this question I'm getting three documents back because I'm just limited here how many documents are going to come back and then I'm going to set up a new shame but this time I'm using pine cone so I'm passing the retriever coming from pine cone and everything else is exactly the same and when we execute that you're going to get Hollywood is going to start using AI to generate scenes etc etc so it's answering that question from the pine cone database that's live so that's it so hopefully this makes sense hopefully this showed you a little bit more about embeddings a little bit more about tokenization a little bit more about how to use land chain to put all of these together I'm going to be recording more videos related to land chain and language models in the coming few weeks so yeah stay tuned if you want to see more of these videos and let me know in the comments if you have any questions or if you have any topics that you would like me to discuss thank you bye bye